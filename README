COLIBRI: Construction as Linguistic Bridges
	Maarten van Gompel 
	proycon@anaproy.nl
	Radboud University Nijmegen

	Licensed under GPLv3


1. ENCODING YOUR CORPUS
----------------------

Encode your plain-text corpus data to a compressed binary form. Make sure your input is tokenised and each sentence is on one line!
 
	$ classencode -f yourcorpus

This results in a class file (yourcorpus.cls) and the encoded corpus (yourcorpus.clsenc). These can be used by the patternfinder. 

To decode an encoded corpus back to it's normal textual form:

 	$ classdecode -f yourcorpus.clsenc -c yourcorpus.cls
 	
If your corpus is not tokenised yet, you can use ucto (distributed seperately), this will also do sentence detection and output one line per sentence. 

	ucto -L en -n untokenisedcorpus.txt > tokenisedcorpus.txt
	
	(for English, several other languages are also supported )

Often, you want to encode a corpus using a pre-existing class file. This occurs especially when you are working with a training set and a separate test set. The initial class file is built on the training set, and it can be reused to encode the test set:

   $ classencode -f trainset
   $ classencode -f testset -c trainset.cls

The first step built trainset.cls and trainset.clsenc , the second step builds testset.clsenc and produces an extended class file testset.cls, which is compatible with trainset.cls and adds only words that were not present in training. 

2. FINDING PATTERNS
-------------------------
 
 1) Make sure to have a properly encoded corpus (see previous section)
 
 2) Invoke the pattern finder your encoded corpus. You can create either an indexed model or an unindexed model, the latter will consume significantly less memory as it does not retain an index to where ngrams/skipgrams occur in the corpus, it does not retain skip content either and is less configurable and limited in use.

  To make an indexed model with skipgrams (-s):	
	 $ patternfinder -f yourcorpus.clsenc -t 10 -s -B -E

  To make an unindexed model add -u:
	 $ patternfinder -u -f yourcorpus.clsenc -t 10 -s -B -E
 
 (-t specified the pruning threshold in number of tokens per construction. To omit skipgrams and do only n-grams, leave -s out. See patternfinder -h for all options. When generating skipgrams, you may want to add -B and -E to omit skipgrams that start and/or end with a skip)

The above steps results in an ngram/skipgram model yourcorpus.indexedpatternmodel.colibri or yourcorpus.unindexedpatternmodel.colibri . These are stored in a binary format for efficient processing and minimal disk/memory space.
 
 3) Decode the model back (to stdout) using the -d and -c options. This will generate a table with all found patterns and associated scores and indices:
 
   For indexed models:
    $ patternfinder -d yourcorpus.indexedpatternmodel.colibri -c yourcorpus.cls > patterns.txt
	
   For unindexed models add -u:
    $ patternfinder -u -d yourcorpus.unindexedpatternmodel.colibri -c yourcorpus.cls > patterns.txt
     
  Note that indexed models can be always read (and decoded) in an unindexed way (with -u), but unindexed models can not be read in an indexed way, as they simply lack indices.
	
 4) Compute coverage statistics using -C , this provides valuable generalised statistics on the model. It only works with indexed models:
 
   $ patternfinder -d yourcorpus.indexedpatternmodel.colibri -C
  
  

	EXAMPLE OUTPUT:

		  COVERAGE REPORT
	----------------------------------
	Total number of tokens:      6258043

		                          TOKENS  COVERAGE  TYPES        TTR     COUNT FREQUENCY
	Total coverage:              6095775    0.9741 298298 123969.960  23769771    1.0000
	Uncovered:                    162268    0.0259

	N-gram coverage:             6095772    0.9741 174363    34.9602  13357746    0.5620
	 1-gram coverage:            6095772    0.9741  16977     0.0028   6095772    0.2565
	 2-gram coverage:            5399143    0.8628  66845     0.0124   4353264    0.1831
	 3-gram coverage:            2298818    0.3673  55001     0.0239   1918446    0.0807
	 4-gram coverage:             615552    0.0984  22227     0.0361    633196    0.0266
	 5-gram coverage:              80184    0.0128   8002     0.0998    213206    0.0090
	 6-gram coverage:              19643    0.0031   3046     0.1551     81033    0.0034
	 7-gram coverage:               5174    0.0008   1414     0.2733     39487    0.0017
	 8-gram coverage:               2732    0.0004    851     0.3115     23342    0.0010

	Skipgram coverage:           2871204    0.4588 123935    23.1670  10412025    0.4380
	 3-skipgram coverage:        1819730    0.2908  23063     0.0127   2088113    0.0878
	 4-skipgram coverage:        1380211    0.2205  19573     0.0142   1494049    0.0629
	 5-skipgram coverage:         757273    0.1210  21957     0.0290   1994542    0.0839
	 6-skipgram coverage:         299785    0.0479  20698     0.0690   1669473    0.0702
	 7-skipgram coverage:         137551    0.0220  18281     0.1329   1525863    0.0642
	 8-skipgram coverage:          81042    0.0130  20363     0.2513   1639985    0.0690


 5) If you work with separate training and test sets. Then you can build a test model as follows: 

  For indexed models:
   $ patternfinder -f trainset.clsenc -t 10 -s -B -E
   $ patternfinder -d trainset.indexedpatternmodel.colibri -f testset.clsenc -t 10 -s -B -E

  For unindexed models:
   $ patternfinder -f trainset.clsenc -t 10 -s -B -E -u
   $ patternfinder -d trainset.unindexedpatternmodel.colibri -f testset.clsenc -t 10 -s -B -E -u
  

 Always make sure that both models use the exact same class file, as seen in the previous section, otherwise this will not work.
 
 The test model that is learned in this step will contain only patterns that were also found during training. 


 


3. BUILDING A GRAPH MODEL    
-------------------------

Indexed pattern models can be used to build more sophisticated graph-based models.

 (to be completed later)

4. TRAINING AN ALIGNMENT MODEL
---------------------------------
 
 1) Encode your source corpus and target corpus (see section 1)
 2) Build an (independent) indexed pattern model for both (see section 2)
 3) For both source and target, build a graph model (see previous section) containing parent relations (-P) and exclusive count data (-X):
    grapher -f source.indexedpatternmodel.colibri -P -X
    grapher -f target.indexedpatternmodel.colibri -P -X
  
  This results in source.graphmodel.colibri and target.graphmodel.colibri . These models can be decoded to a textual representation by passing them to grapher using -d and the cls file using -c . 
 
 4) Train an alignment model using the graph models:
   
 
 
   
  
    
      
 

