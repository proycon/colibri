

<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>Colibri Documentation &mdash; Colibri 0.1 documentation</title>
    
    <link rel="stylesheet" href="_static/default.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '',
        VERSION:     '0.1',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="_static/jquery.js"></script>
    <script type="text/javascript" src="_static/underscore.js"></script>
    <script type="text/javascript" src="_static/doctools.js"></script>
    <link rel="top" title="Colibri 0.1 documentation" href="#" /> 
  </head>
  <body>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li><a href="#">Colibri 0.1 documentation</a> &raquo;</li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body">
            
  <div class="section" id="colibri-documentation">
<h1>Colibri Documentation<a class="headerlink" href="#colibri-documentation" title="Permalink to this headline">¶</a></h1>
<div class="toctree-wrapper compound">
<ul class="simple">
</ul>
</div>
<div class="section" id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Permalink to this headline">¶</a></h2>
<p>Colibri is a collection of software developed for the Ph.D. research project <strong>Constructions as Linguistic Bridges</strong>. This research examines the identification and extraction of aligned constructions or patterns across natural languages, and the usage of such constructions in Machine Translation. The aligned constructions are not identified on the basis of an extensive and explicitly defined grammar or expert database of linguistic knowledge, but rather are <em>implicitly</em> distilled from large amounts of example data. Our notion of constructions is broad and transcends the idea of words or variable-length phrases. We also consider constructions with one or more gaps, as well as context-sensitive machine learning techniques. Our aim is to find new methods that prove beneficial in a Machine Translation setting.</p>
<p>The software consists out of various tools, each of which will be discussed in this documentation. Because of the computational complexity and need to have the software deal as efficiently as possible with time and space constraints, almost everything is written in C++. The exception is the experiment framework surrounding the actual core software; this is written in Python. Depending on the size of your input corpus, certain tasks may take considerable memory. We recommend a Linux machine with at least 8GB RAM.</p>
<p>This documentation will illustrate how to work with the various tools of colibri, as well as elaborate on the implementation of certain key aspects of the software.</p>
</div>
<div class="section" id="installation">
<h2>Installation<a class="headerlink" href="#installation" title="Permalink to this headline">¶</a></h2>
<p>Colibri is hosted on <a class="reference external" href="http://github.com/proycon/colibri/">github</a> and should be retrieved through the versioning control system <tt class="docutils literal"><span class="pre">git</span></tt>. Provided git is installed on your system, this is done as follows:</p>
<div class="highlight-python"><pre>$ git clone git://github.com/proycon/colibri.git</pre>
</div>
<p>You need to compile the software, but in order to do so you must first install the dependency <tt class="docutils literal"><span class="pre">Timbl</span></tt> <a class="reference internal" href="#daelemans2010">[Daelemans2010]</a> ; a tarball is obtainable from <a class="reference external" href="http://ilk.uvt.nl/timbl/">the Timbl website</a> , follow the instructions included with Timbl to install it.</p>
<p>In addition to the C/C++ compiler (<tt class="docutils literal"><span class="pre">gcc</span></tt>), the build process for colibri makes use of <tt class="docutils literal"><span class="pre">autoconf</span></tt> and <tt class="docutils literal"><span class="pre">automake</span></tt>. Make sure these are installed on your system. Also install the package <tt class="docutils literal"><span class="pre">autoconf-archive</span></tt> if available on your distribution. Colibri can now be compiled and installed:</p>
<div class="highlight-python"><pre>$ cd colibri
$ bash bootstrap
$ ./configure
$ make
$ make install</pre>
</div>
<p>You can optionally pass a prefix if you want to install colibri in a different location:</p>
<div class="highlight-python"><pre>$ ./configure --prefix=/usr/local/</pre>
</div>
<p>If you want to make use of the MT experiment framework included as a part of Colibri, you also need the Python Natural Language Processing library (pynlpl) installed. This can be done as follows:</p>
<div class="highlight-python"><pre>$ sudo easy_install pynlpl</pre>
</div>
<p>If <tt class="docutils literal"><span class="pre">easy_install</span></tt> is not available, then first install the <tt class="docutils literal"><span class="pre">python-setuptools</span></tt> package. Or obtain pynlpl manually through <tt class="docutils literal"><span class="pre">git</span></tt>:</p>
<div class="highlight-python"><pre>$ git clone git://github.com/proycon/pynlpl.git</pre>
</div>
<div class="section" id="keeping-colibri-up-to-date">
<h3>Keeping colibri up to date<a class="headerlink" href="#keeping-colibri-up-to-date" title="Permalink to this headline">¶</a></h3>
<p>Colibri is always under heavy development. Update your colibri copy by issuing a git pull:</p>
<div class="highlight-python"><pre>$ git pull</pre>
</div>
<p>And then recompile as per the above instructions.</p>
</div>
<div class="section" id="general-usage-instructions">
<h3>General usage instructions<a class="headerlink" href="#general-usage-instructions" title="Permalink to this headline">¶</a></h3>
<p>Colibri consist of various programs, each of which will output an extensive overview of available parameters if the parameter <tt class="docutils literal"><span class="pre">-h</span></tt> is passed. Each program is designed for a specialised purpose, with specific input and output formats. It is often needed to call multiple programs in succession to obtain the final analysis or model you desire.</p>
</div>
</div>
<div class="section" id="corpus-class-encoding">
<h2>Corpus Class Encoding<a class="headerlink" href="#corpus-class-encoding" title="Permalink to this headline">¶</a></h2>
<div class="section" id="id2">
<h3>Introduction<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h3>
<p>Computation on large datasets begs for solutions to keep memory consumption manageable. Colibri requires that input corpora are converted into a compressed binary form. In this form each word-type in the corpus is represented by a numeric class. Highly frequent word-types get assigned low class numbers and less frequent word-types get higher class numbers. The class is represented in a dynamic-width byte-array, rather than a fixed-width integer. The null byte has special significance and is used as delimiter between tokens. This implies that a class number may never contain a null byte in its representation.</p>
<p>All internal computations of all tools in colibri proceed on this internal representation rather than actual textual strings, keeping running time shorter and memory footprint significantly smaller.</p>
</div>
<div class="section" id="class-encoding-your-corpus">
<h3>Class-encoding your corpus<a class="headerlink" href="#class-encoding-your-corpus" title="Permalink to this headline">¶</a></h3>
<p>When working with colibri, you first want to <strong>class encode</strong> your corpus. This is done by the program <tt class="docutils literal"><span class="pre">classencode</span></tt>. It takes as input a <em>tokenised</em> monolingual corpus in plain text format, containing <em>one sentence per line</em>. Each line should be delimited by a single newline character (unix line endings). Colibri is completely agnostic when it comes to the character encoding of the input. Given a corpus file <tt class="docutils literal"><span class="pre">yourcorpus</span></tt>, class encoding is done as follows:</p>
<div class="highlight-python"><pre>$ classencode -f yourcorpus</pre>
</div>
<p>This results in two files:</p>
<blockquote>
<div><ul class="simple">
<li><tt class="docutils literal"><span class="pre">yourcorpus.cls</span></tt> - This is the class file; it lists all word-types and class numbers.</li>
<li><tt class="docutils literal"><span class="pre">yourcorpus.clsenc</span></tt> - This is the corpus is encoded binary form. It is a lossless compression that is roughly half the size of the original</li>
</ul>
</div></blockquote>
<p>If your corpus is not tokenised yet, you can consider using the tokeniser <a class="reference external" href="http://ilk.uvt.nl/ucto">ucto</a> (not part of colibri), this will also do sentence detection and output one line per sentence:</p>
<div class="highlight-python"><pre>$ ucto -L en -n untokenisedcorpus.txt &gt; tokenisedcorpus.txt</pre>
</div>
<p>The above sample is for English (<tt class="docutils literal"><span class="pre">-L</span> <span class="pre">en</span></tt>), several other languages are also supported.</p>
</div>
<div class="section" id="class-decoding-your-corpus">
<h3>Class-decoding your corpus<a class="headerlink" href="#class-decoding-your-corpus" title="Permalink to this headline">¶</a></h3>
<p>Given an encoded corpus and a class file, the original corpus can always be reconstructed. This we call <em>class decoding</em> and is done using the <tt class="docutils literal"><span class="pre">classdecode</span></tt> program:</p>
<div class="highlight-python"><pre>$ classdecode -f yourcorpus.clsenc -c yourcorpus.cls</pre>
</div>
<p>Output will be to <tt class="docutils literal"><span class="pre">stdout</span></tt>.</p>
</div>
<div class="section" id="class-encoding-with-existing-classes">
<h3>Class-encoding with existing classes<a class="headerlink" href="#class-encoding-with-existing-classes" title="Permalink to this headline">¶</a></h3>
<p>Sometimes you want to encode new data using the same classes already used for another data set. For instance when comparing corpora, it is vital that the same classes are used, i.e. that identical words are assigned identical numerical classes. This also applies when you are working with a training set and a separate test set, or are otherwise interested in a comparative analysis between two comparable datasets. The initial class file is built on the training set, and it can be reused to encode the test set:</p>
<p>You can encode a dataset, here named <tt class="docutils literal"><span class="pre">testset</span></tt> using an existing class file, <tt class="docutils literal"><span class="pre">trainset.cls</span></tt>, as follows:</p>
<div class="highlight-python"><pre>$ classencode -f testset -c trainset.cls</pre>
</div>
<p>This will result in an encoded corpus testset.clsenc and an <em>extended</em> class file testset.cls, which is a superset of the original trainset.cls, adding only those classes that did not yet exist in the training data.</p>
</div>
</div>
<div class="section" id="pattern-finder">
<h2>Pattern Finder<a class="headerlink" href="#pattern-finder" title="Permalink to this headline">¶</a></h2>
<div class="section" id="id3">
<h3>Introduction<a class="headerlink" href="#id3" title="Permalink to this headline">¶</a></h3>
<p>The <tt class="docutils literal"><span class="pre">patternfinder</span></tt> program is used to extract recurring patterns from a monolingual corpus. This results in a pattern model. The extracted patterns are n-grams or skip-grams, where a skip-gram an n-gram with one or more gaps of a predefined size, thus containing unspecified or wildcard tokens. The internal data representation consists of a byte-array where a null byte separates words, each represented by a class number. Gaps are expressed by two consecutive null bytes, the configuration of the gap sizes is stored in a separate byte-array.</p>
<p>The pattern finding algorithm is iterative in nature and it guaranteed to find all n-grams above a specified occurrence threshold and given a maximum size for n. It does so by iterating over the corpus n times, iterating over all possible values for n in ascending order. At each iteration, a sliding window extracts all n-grams in the corpus for the size is question. An n-gram is counted in a hashmap data structure only if both n-1-grams it by definition contains are found during the previous iteration with an occurrence above the set threshold. The exception are unigrams, which are all by definition counted if they reach the threshold, as they are already atomic in nature. At the end of each iteration, n-grams not making it to the occurrence threshold are pruned. This simple iterative technique reduces the memory footprint compared to the more naive approach of immediately storing all in a hashmap, as it prevents the storing of lots of patterns not making the threshold by discarding them at an earlier stage.</p>
<p>At the beginning of each iteration of n, all possible ways in which any n-gram of size n can contain gaps is computed. When an n-gram is found, various skip-grams are tried in accordance with these gap configurations. This is accomplished by &#8216;punching holes&#8217; in the n-gram, resulting in a skip-gram. If all consecutive parts of this skip-gram were counted during previous iterations and thus made the threshold, then the skip-gram as a whole is counted, otherwise it is discarded. After each iteration, pruning again takes places to prune skip-grams that are not frequent enough.</p>
<p>The pattern finder can create either indexed or unindexed models. For indexed models, the precise location of where an n-gram or skipgram instance was found in the corpus is recorded. This comes at the cost of much higher memory usage. Other colibri tools such as the <tt class="docutils literal"><span class="pre">grapher</span></tt> and <tt class="docutils literal"><span class="pre">aligner</span></tt> demand an indexed pattern model as input. For skipgrams in indexed models, the various fillings for the gaps are recorded explicitly. If you are only interested in simple n-gram or simple skip-gram counts, then an unindexed model may suffice.</p>
</div>
<div class="section" id="creating-a-pattern-model">
<h3>Creating a pattern model<a class="headerlink" href="#creating-a-pattern-model" title="Permalink to this headline">¶</a></h3>
<p>First make sure to have class encoded your corpus. Given this encoded corpus, <tt class="docutils literal"><span class="pre">patternfinder</span></tt> can be invoked to produce an indexed pattern model. The occurrence threshold is specified with parameter <tt class="docutils literal"><span class="pre">-t</span></tt>, patterns occuring less will not be counted. The default value is two.  The maximum value for n, i.e. the maximum n-gram/skipgram size, is set using the parameter <tt class="docutils literal"><span class="pre">-l</span></tt>, it defaults to 9:</p>
<div class="highlight-python"><pre>$ patternfinder -f yourcorpus.clsenc -t 10</pre>
</div>
<p>This will result in a pattern model <tt class="docutils literal"><span class="pre">yourcorpus.indexedpatternmodel.colibri</span></tt>. This model is stored in a binary format. To turn it into a human readable presentation it needs to be decoded. The <tt class="docutils literal"><span class="pre">patternfinder</span></tt> program can do this by loading the model using the <tt class="docutils literal"><span class="pre">-d</span></tt> flag, provided you also pass the class file needed for class decoding using <tt class="docutils literal"><span class="pre">-c</span></tt> parameter:</p>
<div class="highlight-python"><pre>$ patternfinder -d yourcorpus.indexedpatternmodel.colibri -c yourcorpus.cls</pre>
</div>
<p>Output will be to <tt class="docutils literal"><span class="pre">stdout</span></tt> in a tab delimited format, facilitating easy parsing. An excerpt follows:</p>
<div class="highlight-python"><pre>#TYPES=89126;TOTALTOKENS=681047
#N      CLASS   OCC.COUNT       TOKENS  COVERAGE        FREQ-ALL        FREQ-G  FREQ-N  SKIPTYPES       ENTROPY REFERENCES
3       the criteria of 3       9       1.321494698603767e-05   2.054269696851421e-06   2.054269696851421e-06   1.43282213423633e-05    0       -       1028:15 4342:43 14772:15
1       picking 3       3       4.404982328679225e-06   2.054269696851421e-06   2.054269696851421e-06   4.502354731524587e-06   0       -       2289:2 5825:20 13913:47
2       and interests   5       10      1.468327442893075e-05   3.423782828085701e-06   3.423782828085701e-06   1.056798703096632e-05   0       -       225:20 2466:47 2796:14 12622:13 21237:22</pre>
</div>
<p>The various columns are:</p>
<ul class="simple">
<li><strong>N</strong> - The length of the n-gram or skipgram in words</li>
<li><strong>Class</strong> - The actual pattern. Gaps in skipgrams are represented as <tt class="docutils literal"><span class="pre">{*x*}</span></tt> where x is a number representing the size of the skip.</li>
<li><strong>Occurrence count</strong> - The absolute number of times this pattern occurs</li>
<li><strong>Tokens</strong> - The absolute number of tokens in the corpus that this pattern covers. Computed as <tt class="docutils literal"><span class="pre">occurrencecount</span> <span class="pre">*</span> <span class="pre">n</span></tt>.</li>
<li><strong>Coverage</strong> - The number of covered tokens, as a fraction of the total number of tokens.</li>
<li><strong>Frequency over all</strong> - The frequency of the pattern, as a fraction of the sum of all n-grams and skipgrams</li>
<li><strong>Frequency within group</strong> - The frequency of the pattern, as a fraction of either all n-grams or all skipgrams, depending on the nature of the pattern.</li>
<li><strong>Frequency within n</strong> - The frequency of the patterns, as a fraction of either all n-grams or all skipgrams of the same size (again depending on the nature of the pattern).</li>
<li><strong>Skip-types</strong> - The number of unique types in the skip content, i.e. the number of unique ways in which gaps can be filled (only for skip-grams, 0 otherwise)</li>
<li><strong>Entropy</strong> - The entropy over the skip content distribution.</li>
<li><strong>References</strong> - A space-delimited list of indices in the corpus that correspond to a occurrence of this pattern. Indices are in the form <tt class="docutils literal"><span class="pre">sentence:token</span></tt> where sentence starts at one and token starts at zero. This column is only available for indexed models.</li>
</ul>
<p>The pattern model created in the previous example did not yet include skip-grams, these have to be explicitly enabled with the <tt class="docutils literal"><span class="pre">-s</span></tt> flag. When this is used, several other options become available for consideration:</p>
<ul class="simple">
<li><tt class="docutils literal"><span class="pre">-B</span></tt> - Do <strong>not</strong> include skipgrams that start with a gap.</li>
<li><tt class="docutils literal"><span class="pre">-E</span></tt> - Do <strong>not</strong> include skipgrams that end with a gap.</li>
<li><tt class="docutils literal"><span class="pre">-T</span> <span class="pre">[value]</span></tt> - Skip content threshold value. Only skip content that occurs at least this many times will be considered. This is an extra occurrence threshold that applies to the amount of occurrences that can fill the gaps. The defaut is the same as the over-all threshold <tt class="docutils literal"><span class="pre">-t</span></tt>. Moreover, this value</li>
<li><tt class="docutils literal"><span class="pre">-S</span> <span class="pre">[value]</span></tt> - Type threshold value. Skipgrams will only be included if they have at least this many unique types as skip content, i.e. if there are at least this many options to fill the gaps. The default is two. Note that <tt class="docutils literal"><span class="pre">-S</span></tt> applies to types whereas <tt class="docutils literal"><span class="pre">-T</span></tt> applies to tokens. The default value is two.</li>
</ul>
<p>Here is an example of generating an indexed pattern model including skipgrams:</p>
<div class="highlight-python"><pre>$ patternfinder -f yourcorpus.clsenc -t 10 -s -S 3 -B -E</pre>
</div>
<p>If you want to generate unindexed models, simply add the flag <tt class="docutils literal"><span class="pre">-u</span></tt>. Do note that for unindexed models the parameter <tt class="docutils literal"><span class="pre">-S</span></tt> is set unchangeably to two and and <tt class="docutils literal"><span class="pre">-T</span></tt> is set fixed to the same value as <tt class="docutils literal"><span class="pre">-t</span></tt>. When decoding an unindexed model, you explicitly need to add the <tt class="docutils literal"><span class="pre">-u</span></tt> flag, as shown in the next example.  Note that indexed models can also be read (and decoded) in an unindexed way (with the <tt class="docutils literal"><span class="pre">-u</span></tt> flag); but unindexed models can not be read in an indexed way, as they simply lack indices:</p>
<div class="highlight-python"><pre>$ patternfinder -d yourcorpus.unindexedpatternmodel.colibri -c yourcorpus.cls -u
$ patternfinder -d yourcorpus.indexedpatternmodel.colibri -c yourcorpus.cls -u</pre>
</div>
</div>
<div class="section" id="training-and-testing-coverage">
<h3>Training and testing coverage<a class="headerlink" href="#training-and-testing-coverage" title="Permalink to this headline">¶</a></h3>
<p>An important quality of pattern models lies in the fact that pattern models can be compared. More specifically, you can train a pattern model on a corpus and test it on another corpus, which yields another pattern model containing only those patterns that occur in both training and test data. The difference in count, frequency and coverage can then be easily be compared.</p>
<p>Make sure to use the same class file for all datasets you are comparing. Instructions for this were given in <em class="xref std std-ref">classencodetraintest</em>:</p>
<div class="highlight-python"><pre>$ patternfinder -f trainset.clsenc -t 10 -s -B -E
$ patternfinder -d trainset.indexedpatternmodel.colibri -f testset.clsenc -t 10 -s -B -E</pre>
</div>
<p>This results in a model <tt class="docutils literal"><span class="pre">testset.colibri.indexedpatternmodel</span></tt>. This model can be used to generate a coverage report using the <tt class="docutils literal"><span class="pre">-C</span></tt> flag:</p>
<div class="highlight-python"><pre>$ patternfinder -d yourcorpus.indexedpatternmodel.colibri -C

     EXAMPLE OUTPUT:

               COVERAGE REPORT
     ----------------------------------
     Total number of tokens:      6258043

                                       TOKENS  COVERAGE  TYPES        TTR     COUNT FREQUENCY
     Total coverage:              6095775    0.9741 298298 123969.960  23769771    1.0000
     Uncovered:                    162268    0.0259

     N-gram coverage:             6095772    0.9741 174363    34.9602  13357746    0.5620
      1-gram coverage:            6095772    0.9741  16977     0.0028   6095772    0.2565
      2-gram coverage:            5399143    0.8628  66845     0.0124   4353264    0.1831
      3-gram coverage:            2298818    0.3673  55001     0.0239   1918446    0.0807
      4-gram coverage:             615552    0.0984  22227     0.0361    633196    0.0266
      5-gram coverage:              80184    0.0128   8002     0.0998    213206    0.0090
      6-gram coverage:              19643    0.0031   3046     0.1551     81033    0.0034
      7-gram coverage:               5174    0.0008   1414     0.2733     39487    0.0017
      8-gram coverage:               2732    0.0004    851     0.3115     23342    0.0010

     Skipgram coverage:           2871204    0.4588 123935    23.1670  10412025    0.4380
      3-skipgram coverage:        1819730    0.2908  23063     0.0127   2088113    0.0878
      4-skipgram coverage:        1380211    0.2205  19573     0.0142   1494049    0.0629
      5-skipgram coverage:         757273    0.1210  21957     0.0290   1994542    0.0839
      6-skipgram coverage:         299785    0.0479  20698     0.0690   1669473    0.0702
      7-skipgram coverage:         137551    0.0220  18281     0.1329   1525863    0.0642
      8-skipgram coverage:          81042    0.0130  20363     0.2513   1639985    0.0690</pre>
</div>
<p>The coverage report shows the number of tokens covered by n-grams or skipgrams, split down in different categories for each <em>n</em>. The <em>coverage</em> column expresses this value as a fraction of the total number of tokens in the corpus. It shows how much of the test set is covered by the training set. The <em>types</em> column expresses how many unique patterns exist in the category. The <em>count</em> columns expresses the absolute occurrence count and the <em>frequency</em> column expresses the over-all frequency. The fact that the occurrence count can be higher than the absolute number of tokens covered is due to the fact that n-grams and skipgrams inherently show considerable overlap when grouped together.</p>
<p>A coverage report can be generated for any <em>indexed</em> pattern model; including models not generated on a separate test set.</p>
</div>
<div class="section" id="query-mode">
<h3>Query mode<a class="headerlink" href="#query-mode" title="Permalink to this headline">¶</a></h3>
<p>The pattern finder has query mode which allows you to quickly extract patterns from test sentences or fragments thereof. The query mode is invoked by loading a pattern model (<tt class="docutils literal"><span class="pre">-d</span></tt>), a class file (<tt class="docutils literal"><span class="pre">-c</span></tt>) and the <tt class="docutils literal"><span class="pre">-Q</span></tt> flag. The query mode can be run interactively as it takes input from <tt class="docutils literal"><span class="pre">stdin</span></tt>, one <em>tokenised</em> sentence per line. The following example illustrates this, the sentence <em>&#8220;This is a test .&#8221;</em> was typed as input:</p>
<div class="highlight-python"><pre>$ patternfinder -d europarl25k-en.indexedpatternmodel.colibri -c europarl25k-en.cls -Q
Loading model
Loading class decoder europarl25k-en.cls
Loading class encoder europarl25k-en.cls
Starting query mode:
1&gt;&gt; This is a test .
1:0     This    1085    0.001593135275538986
1:0     This is 395     0.001159978679885529
1:0     This is a       64      0.0002819188690354704
1:0     This {*1*} a    66      0.0002907288336928288
1:1     is      10570   0.0155202210713798
1:1     is a    947     0.002781012176839484
1:2     a       10272   0.01508265949339767
1:2     a test  2       5.8733097715723e-06
1:3     test    30      4.404982328679225e-05
1:4     .       23775   0.03490948495478285</pre>
</div>
<p>The output starts with an index in the format <tt class="docutils literal"><span class="pre">sentence:token</span></tt>, the pattern found, and the next two values are the absolute occurrence count and the coverage ratio.</p>
</div>
</div>
<div class="section" id="graph-models">
<h2>Graph Models<a class="headerlink" href="#graph-models" title="Permalink to this headline">¶</a></h2>
<div class="section" id="id4">
<h3>Introduction<a class="headerlink" href="#id4" title="Permalink to this headline">¶</a></h3>
<p>A pattern model contains a wide variety of patterns; a graph model goes a step further by making explicit relationships between the various patterns in this model. These relationships can be visualised as a directed graph, in which the nodes represent the various patterns (n-grams and skipgrams), and the edges represent the relations. The following relations are distinguished; note that as the graph is directed relations often come in pairs; one relationship for each direction:</p>
<ul class="simple">
<li><strong>Subsumption relations</strong> - Patterns that are subsumed by larger patterns are called <em>children</em>, the larger patterns are called <em>parents</em>. These are the two subsumption relations that can be extracted from an indexed pattern model.</li>
<li><strong>Successor relations</strong>  - If a pattern A and a pattern B form part of a combined pattern A B, then the two patterns are in a successor/predecessor relationship.</li>
<li><strong>Template relations</strong> - Template relations indicate abstraction and go from n-grams to skipgrams. An example of a template relation is <tt class="docutils literal"><span class="pre">to</span> <span class="pre">be</span> <span class="pre">or</span> <span class="pre">not</span> <span class="pre">to</span> <span class="pre">be</span></tt> to <tt class="docutils literal"><span class="pre">to</span> <span class="pre">be</span> <span class="pre">{*1*}</span> <span class="pre">not</span> <span class="pre">{*1*}</span> <span class="pre">be</span></tt>. The reverse direction is called the instance-relationship, as an specific n-gram is an instance of a more abstract template.</li>
<li><strong>Skip content relations</strong> - Relations between patterns that can be used to fill the gaps of higher patterns are called skip content relations. These can go in two directions; skipgram to skip content and skip content to skipgram. Example: <tt class="docutils literal"><span class="pre">to</span></tt> is a in a skip-content to skipgram relationship with  <tt class="docutils literal"><span class="pre">to</span> <span class="pre">be</span> <span class="pre">{*1*}</span> <span class="pre">not</span> <span class="pre">{*1*}</span> <span class="pre">be</span></tt>.</li>
</ul>
<p>In addition to the relations, a graph model can also compute a so-called <em>exclusivity count</em> and <em>exclusivity ratio</em> for each pattern. The exclusivity count of a pattern is the number of times the pattern occurs in the data <em>without</em> being subsumed by a larger found pattern. This exclusivity ratio is the exclusivity count as a fraction of the total occurrence count for the pattern. An exclusivity ratio of one indicates that the pattern is fully exclusive, meaning it is not subsumed by higher-order patterns. This notion of exclusivity may be of use in assessing compositionality of patterns.</p>
</div>
<div class="section" id="computing-a-graph-model">
<h3>Computing a graph model<a class="headerlink" href="#computing-a-graph-model" title="Permalink to this headline">¶</a></h3>
<p>The <tt class="docutils literal"><span class="pre">grapher</span></tt> program computes a graph model on the basis of an <strong>indexed</strong> pattern model created with <tt class="docutils literal"><span class="pre">patternfinder</span></tt>. When computing a model, you need to explicitly specify which relations you desire to extract and include in your model. The more relations you include, the more memory will be required. To keep the models as small as possible, it is recommended to include only the relations you need. The following flags are available:</p>
<ul class="simple">
<li><tt class="docutils literal"><span class="pre">-P</span></tt> - Compute/load subsumption relations from children to parents (reverse of -C)</li>
<li><tt class="docutils literal"><span class="pre">-C</span></tt> - Compute/load subsumption relations from parents to children (reverse of -P)</li>
<li><tt class="docutils literal"><span class="pre">-S</span></tt> - Compute/load subsumption skipgram to skipcontent relations</li>
<li><tt class="docutils literal"><span class="pre">-s</span></tt> - Compute/load subsumption skip-content to skipgram relations (reverse of -S)</li>
<li><tt class="docutils literal"><span class="pre">-L</span></tt> - Compute/load subsumption predecessor relations (constructions to the left)</li>
<li><tt class="docutils literal"><span class="pre">-R</span></tt> - Compute/load subsumption sucessor relations (constructions to the right)</li>
<li><tt class="docutils literal"><span class="pre">-T</span></tt> - Compute/load subsumption template relations</li>
<li><tt class="docutils literal"><span class="pre">-I</span></tt> - Compute/load subsumption instance relations (reverse of -T)</li>
<li><tt class="docutils literal"><span class="pre">-a</span></tt> - Compute/load subsumption all relations</li>
<li><tt class="docutils literal"><span class="pre">-X</span></tt> - Compute/load exclusivity count/ratios</li>
</ul>
<p>The indexed pattern model that acts as input is specified using the <tt class="docutils literal"><span class="pre">-f</span></tt> flag. The following example generates a graph model with all relations:</p>
<div class="highlight-python"><pre>$ grapher -f yourcorpus.indexedpatternmodel.colibri -a</pre>
</div>
<p>The graph model will be stored in binary form, in the file <tt class="docutils literal"><span class="pre">yourcorpus.graphpatternmodel.colibri</span></tt>.</p>
</div>
<div class="section" id="viewing-and-querying-a-graph-model">
<h3>Viewing and querying a graph model<a class="headerlink" href="#viewing-and-querying-a-graph-model" title="Permalink to this headline">¶</a></h3>
<p>To decode this binary graph model into human readable form, read it in using the <tt class="docutils literal"><span class="pre">-d</span></tt> flag and pass a class file. In addition, you again need to pass what relations you want to load, as it is also possible to only load a subset of the relations. Simply use the <tt class="docutils literal"><span class="pre">-a</span></tt> flag if you want to load and output relations existing in the model:</p>
<div class="highlight-python"><pre>$ grapher -d yourcorpus.graphpatternmodel.colibri -c yourcorpus.cls -a</pre>
</div>
<p>This will result in output to <tt class="docutils literal"><span class="pre">stdout</span></tt> in a tab-separated format, as illustrated below:</p>
<div class="highlight-python"><pre>#N      VALUE   OCC.COUNT       TOKENS  COVERAGE        XCOUNT  XRATIO  PARENTS CHILDREN        TEMPLATES       INSTANCES       SKIPUSAGE       SKIPCONTENT     SUCCESSORS      PREDECESSORS
4       the summer break the    2       8       1.17466195431446e-05    0       0       1       8       1       0       0       0       0       1
2       that sustainable        2       4       5.8733097715723e-06     0       0       1       2       0       0       0       0       0       0
2       Kosovo ,        20      40      5.8733097715723e-05     4       0.2     5       2       0       0       1       0       0       1
2       which refer     2       4       5.8733097715723e-06     0       0       2       2       0       0       0       0       1       0
2       relief ,        2       4       5.8733097715723e-06     2       1       0       2       0       0       0       0       0       0
6       rule of law and respect for     3       18      2.642989397207535e-05   1       0.3333333333333333      2       20      0       0       1       0       0       2</pre>
</div>
<p>In the above example, only the number of relations for each type is shown, if you want to view the actual relations, you need to instruct <tt class="docutils literal"><span class="pre">grapher</span></tt> to output the the whole graph by adding the <tt class="docutils literal"><span class="pre">-g</span></tt> flag:</p>
<div class="highlight-python"><pre>$ grapher -d yourcorpus.graphpatternmodel.colibri -c yourcorpus.cls -a -g


      #N      VALUE   OCC.COUNT       TOKENS  COVERAGE        XCOUNT  XRATIO  PARENTS CHILDREN        TEMPLATES       INSTANCES       SKIPUSAGE       SKIPCONTENT     SUCCESSORS      PREDECESSORS
      4       the summer break the    2       8       1.17466195431446e-05    0       0       1       8       1       0       0       0       0       1
      Parent relations - 1
                  before the summer break the     2       10      1.468327442893075e-05   2       1
      Child relations - 8
                  the     44027   44027   0.06464605232825341     90      0.002044200149908011
                  the summer      15      30      4.404982328679225e-05   5       0.3333333333333333
                  the summer break        2       6       8.80996465735845e-06    0       0
                  summer  30      30      4.404982328679225e-05   3       0.1
                  break   17      17      2.496156652918227e-05   2       0.1176470588235294
                  break the       5       10      1.468327442893075e-05   1       0.2
                  summer break the        2       6       8.80996465735845e-06    0       0
                  summer break    2       4       5.8733097715723e-06     0       0
      Predecessor relations - 1
                  before  357     357     0.0005241928971128277   36      0.1008403361344538
      Templates - 1
                  the {*2*} the   3266    13064   0.01918222971395513     3266    1</pre>
</div>
<p>Outputting the whole graph may however produce a lot of unwanted output. Often you want to query your graph model for only one pattern. This is done with the <tt class="docutils literal"><span class="pre">-q</span></tt> parameter. In the following example we query our model for the pattern &#8220;summer&#8221;:</p>
<div class="highlight-python"><pre>$ grapher -d yourcorpus.graphpatternmodel.colibri -c yourcorpus.cls -a -q "summer"

Outputting graph for "summer"
Query:
        summer  30      30      4.40498e-05     3       0.1
Parent relations - 16
        summer break    2       4       5.87331e-06     0       0
        after the summer        2       6       8.80996e-06     2       1
        the summer break the    2       8       1.17466e-05     0       0
        this summer .   5       15      2.20249e-05     5       1
        before the summer break 2       8       1.17466e-05     0       0
        of this summer  2       6       8.80996e-06     2       1
        the summer .    5       15      2.20249e-05     5       1
        before the summer       5       15      2.20249e-05     3       0.6
        the summer      15      30      4.40498e-05     5       0.333333
        before the summer break the     2       10      1.46833e-05     2       1
        this summer     11      22      3.23032e-05     2       0.181818
        the summer break        2       6       8.80996e-06     0       0
        this summer ,   4       12      1.76199e-05     4       1
        summer .        10      20      2.93665e-05     0       0
        summer ,        5       10      1.46833e-05     1       0.2
        summer break the        2       6       8.80996e-06     0       0
Successor relations - 1
        break the       5       10      1.46833e-05     1       0.2
Skipusage - 3
        this {*1*} ,    585     1755    0.00257691      585     1
        this {*1*} .    537     1611    0.00236548      537     1
        the {*1*} .     1496    4488    0.00658985      1496    1</pre>
</div>
<p>It is possible to actually visualise the graph, <tt class="docutils literal"><span class="pre">grapher</span></tt> can output to the so-called dot-format used by the open-source graph visualisation software <a class="reference external" href="http://www.graphviz.org">graphviz</a>. You can output to this format by specifying the <tt class="docutils literal"><span class="pre">-G</span></tt> flag. This works both with and without <tt class="docutils literal"><span class="pre">-q</span></tt>, but if you do not specify a query the graph may turn out to be too huge to visualise:</p>
<div class="highlight-python"><pre>$ grapher -d yourcorpus.graphpatternmodel.colibri -c yourcorpus.cls -a -q "summer" -G &gt; summer.dot</pre>
</div>
<p>Graphviz will do the actual conversion to an image file, such as png:</p>
<div class="highlight-python"><pre>$ dot -Tpng summer.dot -o summer.png</pre>
</div>
<p>This generated the following image:</p>
<img alt="_images/summer.png" src="_images/summer.png" />
</div>
</div>
<div class="section" id="alignment-models">
<h2>Alignment Models<a class="headerlink" href="#alignment-models" title="Permalink to this headline">¶</a></h2>
<div class="section" id="id5">
<h3>Introduction<a class="headerlink" href="#id5" title="Permalink to this headline">¶</a></h3>
<p>An alignment model establishes a translation from patterns in one model to patterns in another. Each alignment has an associated score, or a vector of multiple scores, expressing the quality of the alignment. Alignments can currently be computed in three ways, all of which are unsupervised methods, and the of which the last method is the superior one:</p>
<ul class="simple">
<li><strong>Co-occurrence (Jaccard)</strong> - Alignments are established according to simple Jaccard co-occurrence</li>
<li><strong>Expectation Maximisation</strong> - Alignments between patterns are computed in a fashion similar to IBM Model 1, using Expectation Maximisation. Computation proceeds over the matrix of all patterns, rather than a matrix of mere words as in IBM Model 1.</li>
<li><strong>GIZA Word Alignments</strong> - Alignments are established on the basis of word-alignments computed with <tt class="docutils literal"><span class="pre">GIZA++</span></tt>.</li>
</ul>
<p>The pattern models have to be generated on the basis of a parallel corpus. For colibri a parallel corpus consists of two corpus files, one for each language. The sentences on the n-th line of each of the  corpus files corresponds and should be translations of each other. Pattern and graph models can then be generated separately on both of these corpora. An indexed pattern model, or derived graph model, is required as input for the <tt class="docutils literal"><span class="pre">aligner</span></tt> program.</p>
</div>
<div class="section" id="co-occurrence">
<h3>Co-occurrence<a class="headerlink" href="#co-occurrence" title="Permalink to this headline">¶</a></h3>
<p>Alignments computed solely on the basis of sentence co-occurrence are fairly weak. For all patterns that co-occur in at least one sentence, a Jaccard co-occurrence score is computed as <img class="math" src="_images/math/68a10b058e13dc75b5a82d15c24b224ee0cd135e.png" alt="\frac{|s \cap t|}{|s \cup t|}"/>, where <em>s</em> and <em>t</em> are sets of sentence numbers in which the pattern occurs.</p>
<p>In the following example we translate French to English and assume pattern models have been computed already. Invoke the <tt class="docutils literal"><span class="pre">aligner</span></tt> program as follows, the <tt class="docutils literal"><span class="pre">-J</span></tt> flag chooses Jaccard co-occurrence:</p>
<div class="highlight-python"><pre>$ aligner -s fr.indexedpatternmodel.colibri -t en.indexedpatternmodel.colibri -J</pre>
</div>
<p>This will result in an output file <tt class="docutils literal"><span class="pre">alignmodel.colibri</span></tt>, which is in a binary format. If you want an alternative output filename you can specify it using the <tt class="docutils literal"><span class="pre">-o</span></tt> parameter.</p>
<p>Several other parameters adjust the behaviour of the alignment algorithm and output:</p>
<ul class="simple">
<li><tt class="docutils literal"><span class="pre">-p</span> <span class="pre">[n]</span></tt> - Pruning value. Alignments with co-occurrence scores lower than <em>n</em> will be pruned (prior to any normalisation)</li>
<li><tt class="docutils literal"><span class="pre">-z</span></tt> - Disable normalisation. By default, the aligner will normalise all scores, if you want to retain the exact co-occurrence scores, pass this flag</li>
<li><tt class="docutils literal"><span class="pre">-b</span> <span class="pre">[n]</span></tt> - Best alignments only. Only the best <em>n</em> target alignments for a source pattern will be retained, others are pruned.</li>
<li><tt class="docutils literal"><span class="pre">-N</span></tt> - Do not attempt to directly align any skipgrams</li>
</ul>
</div>
<div class="section" id="expectation-maximisation">
<h3>Expectation Maximisation<a class="headerlink" href="#expectation-maximisation" title="Permalink to this headline">¶</a></h3>
<p>This is an experimental method for computing alignments <em>directly</em> on the basis of the patterns. It is modelled after IBM Model 1 and uses the Expectation Maximisation algorithm to iteratively optimise the model&#8217;s parameters.  The pseudo-code for the EM algorithm applied to this model is as follows:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">initialize</span> <span class="n">t</span><span class="p">(</span><span class="n">t</span><span class="o">|</span><span class="n">s</span><span class="p">)</span> <span class="n">uniformly</span>
<span class="n">do</span> <span class="n">until</span> <span class="n">convergence</span>
       <span class="nb">set</span> <span class="n">count</span><span class="p">(</span><span class="n">t</span><span class="o">|</span><span class="n">s</span><span class="p">)</span> <span class="n">to</span> <span class="mi">0</span> <span class="k">for</span> <span class="nb">all</span> <span class="n">t</span><span class="p">,</span><span class="n">s</span>
       <span class="nb">set</span> <span class="n">total</span><span class="p">(</span><span class="n">s</span><span class="p">)</span> <span class="n">to</span> <span class="mi">0</span> <span class="k">for</span> <span class="nb">all</span> <span class="n">s</span>
   <span class="k">for</span> <span class="nb">all</span> <span class="n">sentence</span> <span class="n">pairs</span> <span class="p">(</span><span class="n">t_s</span><span class="p">,</span><span class="n">s_s</span><span class="p">)</span>
      <span class="nb">set</span> <span class="n">total_s</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="o">=</span> <span class="mi">0</span> <span class="k">for</span> <span class="nb">all</span> <span class="n">t</span>
      <span class="k">for</span> <span class="nb">all</span> <span class="n">patterns</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">t_s</span>
         <span class="k">for</span> <span class="nb">all</span> <span class="n">patterns</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">s_s</span>
           <span class="n">total_s</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="o">+=</span> <span class="n">t</span><span class="p">(</span><span class="n">t</span><span class="o">|</span><span class="n">s</span><span class="p">)</span>
      <span class="k">for</span> <span class="nb">all</span> <span class="n">patterns</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">t_s</span>
          <span class="k">for</span> <span class="nb">all</span> <span class="n">patterns</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">s_s</span>
             <span class="n">count</span><span class="p">(</span><span class="n">t</span><span class="o">|</span><span class="n">s</span><span class="p">)</span> <span class="o">+=</span> <span class="n">t</span><span class="p">(</span><span class="n">t</span><span class="o">|</span><span class="n">s</span><span class="p">)</span> <span class="o">/</span> <span class="n">total_s</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>
             <span class="n">total</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>   <span class="o">+=</span> <span class="n">t</span><span class="p">(</span><span class="n">t</span><span class="o">|</span><span class="n">s</span><span class="p">)</span> <span class="o">/</span> <span class="n">total_s</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>
   <span class="k">for</span> <span class="nb">all</span> <span class="n">s</span>
     <span class="k">for</span> <span class="nb">all</span> <span class="n">t</span>
        <span class="n">t</span><span class="p">(</span><span class="n">t</span><span class="o">|</span><span class="n">s</span><span class="p">)</span> <span class="o">=</span> <span class="n">count</span><span class="p">(</span><span class="n">t</span><span class="o">|</span><span class="n">s</span><span class="p">)</span> <span class="o">/</span> <span class="n">total</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
</pre></div>
</div>
<p>In the following example we translate French to English and assume pattern models have been computed already. Invoke the <tt class="docutils literal"><span class="pre">aligner</span></tt> program as follows, the <tt class="docutils literal"><span class="pre">-E</span></tt> flag chooses Expectation Maximisation:</p>
<div class="highlight-python"><pre>$ aligner -s fr.indexedpatternmodel.colibri -t en.indexedpatternmodel.colibri -E</pre>
</div>
<p>This will result in an output file <tt class="docutils literal"><span class="pre">alignmodel.colibri</span></tt>, which is in a binary format. If you want an alternative output filename you can specify it using the <tt class="docutils literal"><span class="pre">-o</span></tt> parameter.</p>
<p>Several other parameters adjust the behaviour of the EM alignment algorithm and output:</p>
<ul class="simple">
<li><tt class="docutils literal"><span class="pre">-P</span> <span class="pre">[n]</span></tt> - Probability pruning value. Alignments with a probability lower than <em>n</em> will be pruned</li>
<li><tt class="docutils literal"><span class="pre">-M</span> <span class="pre">[n]</span></tt> - Maximum number of iterations (default: 10000)</li>
<li><tt class="docutils literal"><span class="pre">-v</span> <span class="pre">[n]</span></tt> - Minimum delta-value when convergence is considered to have been reached (default: 0.001)</li>
<li><tt class="docutils literal"><span class="pre">-z</span></tt> - Disable normalisation. By default, the aligner will normalise all scores, if you want to retain the exact co-occurrence scores, pass this flag</li>
<li><tt class="docutils literal"><span class="pre">-b</span> <span class="pre">[n]</span></tt> - Best alignments only. Only the best <em>n</em> target alignments for a source pattern will be retained, others are pruned.</li>
<li><tt class="docutils literal"><span class="pre">-N</span></tt> - Do not attempt to directly align any skipgrams, skipgrams tend to worsen alignment quality significantly.</li>
<li><tt class="docutils literal"><span class="pre">--null</span></tt> - Take into account zero-fertility words in EM</li>
<li><tt class="docutils literal"><span class="pre">-I</span> <span class="pre">1</span></tt> - Compute bi-directional alignments and use one joint score. This will run the algorithm twice, once for each direction, and compute the intersection of the results.</li>
<li><tt class="docutils literal"><span class="pre">-I</span> <span class="pre">2</span></tt> - Compute bi-directional alignments and use two separate scores, representing p(t|s) and p(s|t). This will run the algorithm twice, once for each direction, and compute the intersection of the results.</li>
</ul>
<p>Instead of uniform initiatisation, this method can also be initialised using the co-occurrence method laid out in the previous section. Simply add the <tt class="docutils literal"><span class="pre">-J</span></tt> parameter to achieve this.</p>
</div>
<div class="section" id="giza-alignment">
<h3>GIZA Alignment<a class="headerlink" href="#giza-alignment" title="Permalink to this headline">¶</a></h3>
<p><tt class="docutils literal"><span class="pre">GIZA++</span></tt> is open-source software for the computation of word alignment models according to the IBM Models and HMM models (see <a class="reference external" href="http://code.google.com/p/giza-pp/">GIZA++</a>) <a class="reference internal" href="#ochney2003">[OchNey2003]</a> . The <tt class="docutils literal"><span class="pre">aligner</span></tt> program can use the models produced by GIZA++ and extract aligned pairs of phrases. Two GIZA models (<tt class="docutils literal"><span class="pre">*.A3.final</span></tt>) are required, one for each translation direction. The extraction algorithm iterates over all sentence pairs in the GIZA models, these sentence pairs contain information in the form of what word-index of the source sentence is aligned to what word-index of the target sentence, and vice versa for the reverse model. Given such a bidirectional pair of alignments, the algorithm first collects all relevant patterns and for each possible combination it computes whether the word-alignment indices from the GIZA models support the alignment of the patterns.  The criteria for whether an alignment between patterns is supported by the word alignments are:</p>
<ul class="simple">
<li>Is the first word of the source pattern properly aligned according to the bidirectional intersection of the word alignments?</li>
<li>Is the last word of the source pattern properly aligned according to the bidirectional intersection of the word alignments?</li>
<li>Is the first word of the target pattern properly aligned according to the bidirectional intersection of the word alignments?</li>
<li>Is the last word of the target pattern properly aligned according to the bidirectional intersection of the word alignments?</li>
<li>Are none of the source words aligned to target words outside the target pattern?</li>
<li>Are none of the target words aligned to source words outside the source pattern?</li>
</ul>
<p>Of all target patterns (if any) meeting this criteria for a given source pattern, only the strongest one is chosen. An alignment strength score is computed to represent how well an alignment is supported by word alignments. This is not to be confused with the actual alignment probability. This score is the number of words that aligns properly, as a fraction of the longest pattern size of the pair. Only alignments that reach a certain threshold will be aligned. If the score is not perfect (&lt; 0), points from the union of the two word alignment directions will be considered and added to the score as well, however, these alignments carry less weight than intersection alignments (four times less by default).</p>
<p>This extraction algorithm is implemented as follows, given word alignments for source-to-target (<tt class="docutils literal"><span class="pre">sentence_s</span></tt>) and target-to-source (<tt class="docutils literal"><span class="pre">sentence_t</span></tt>):</p>
<div class="highlight-python"><pre>patterns_t = all patterns in sentence_t
for all words word_s in sentence_s:
    patterns_s = find patterns BEGINNING WITH word_s
    for all patterns pattern_s in patterns_s:
        bestscore = 0
        for all patterns pattern_t in patterns_t:
            aligned = 0
            halfaligned = 0
            firstsourcealigned = false
            lastsourcealigned = false
            firsttargetaligned = false
            lasttargetaligned = false
            for for all indices (alignedsourceindex, alignedtargetindex) in intersection:
                if alignedsourceindex not in pattern_s or alignedtargetindex not in pattern_t:
                    aligned--; break;
                else:
                    aligned++;
                    if alignedsourceindex == sourceindex: firstsourcealigned = true
                    if alignedsourceindex == sourceindex + patternsize_s: lastsourcealigned = true
                    if alignedtargetindex == targetindex: firstsourcealigned = true
                    if alignedtargetindex == targetindex + patternsize_t: lastsourcealigned = true


            if ((aligned &lt; 0) || (!firstaligned) || (!lastaligned)) break;

            maxpatternsize = max(|pattern_s|,|pattern_t|)
            score = aligned / maxpatternsize

            if (score &lt; 1):
                for alignedsourceindex, alignedtargetindex in union:
                    if (alignedsourceindex in pattern_s and alignedtargetindex not in pattern_t) or (alignedsourceindex not in pattern_s and alignedtargetindex in pattern_t):
                        halfaligned++;
                if halfaligned:
                    score = score + halfaligned / (maxpatternsize*4)
                    if (score &gt; 1) score = 1

           if score &gt; bestscore:
                bestscore = score
                bestpattern_t = pattern_t</pre>
</div>
<p>In the following example we translate French to English and assume pattern models have been computed already. Invoke the <tt class="docutils literal"><span class="pre">aligner</span></tt> program as follows, the <tt class="docutils literal"><span class="pre">-W</span></tt> flag chooses GIZA extraction and takes as parameters the two GIZA <tt class="docutils literal"><span class="pre">A3.final</span></tt> models (order matters!) separated by a colon. It is also necessary to pass the class the class file for both source (<tt class="docutils literal"><span class="pre">-S</span></tt>) and target language (<tt class="docutils literal"><span class="pre">-T</span></tt>), as the GIZA models do not use the colibri class encodings and thus need to be interpreted on the fly:</p>
<div class="highlight-python"><pre>$ aligner -s fr.indexedpatternmodel.colibri -t en.indexedpatternmodel.colibri -W fr-en.A3.final:en-fr.A3.final -S fr.cls -T en.cls</pre>
</div>
<p>Several other parameters adjust the behaviour of the EM alignment algorithm and output:</p>
<ul class="simple">
<li><tt class="docutils literal"><span class="pre">-a</span> <span class="pre">[n]</span></tt> - Alignment strength, value between 0 and 1. Determines how strong word alignments have to be if phrases are to be extracted, weak alignments will not be considered.</li>
<li><tt class="docutils literal"><span class="pre">-p</span> <span class="pre">[n]</span></tt>-  Prune all alignments with a jaccard co-occurence score lower than specified (0 &lt;= x &lt;= 1). Uses heuristics to prune, final probabilities may turn out lower than they would otherwise be</li>
<li><tt class="docutils literal"><span class="pre">-c</span> <span class="pre">[n]</span></tt> - Prune phrase pairs that occur less than specified, here <em>n</em> is an integer representing the absolute count required for the phrase pair as a whole.</li>
<li><tt class="docutils literal"><span class="pre">-I</span> <span class="pre">1</span></tt> - Compute bi-directional alignments and use one joint score. This does <em>not</em> run the algorithm twice, but is directly integrated.</li>
<li><tt class="docutils literal"><span class="pre">-I</span> <span class="pre">2</span></tt> - Compute bi-directional alignments and use two separate scores, representing p(t|s) and p(s|t). This does <em>not</em> run the algorithm twice, but is directly integrated.</li>
</ul>
<p>This alignment method replaces unsupervised word alignment symmetrisations heuristics like <tt class="docutils literal"><span class="pre">grow-diag</span></tt>, <tt class="docutils literal"><span class="pre">grow-diag-final</span></tt> [OchNey2003].</p>
</div>
<div class="section" id="skipgram-alignment">
<h3>Skipgram alignment<a class="headerlink" href="#skipgram-alignment" title="Permalink to this headline">¶</a></h3>
<p>The alignment of skipgrams poses extra challenges. The three above alignment methods function much worse as soon as skipgrams are included. Therefore, an experimental alignment algorithm has been implemented to extract skipgrams on the basis of a graph model with instance and template relations. The algorithm starts with the n-grams that have been aligned by any of the three aforementioned algorithms, it then determines whether multiple target n-grams for a given source n-gram share a common template (i.e. a skip-gram that is an abstraction of a pattern). The template relation is a transitive relation, so recursion is applied to collect all possible templates, these can subsequently be clustered in one or more clusters, each cluster being completely independent of the other, not sharing any template or instance relationships. From each cluster only the best match, according to Jaccard co-occurrence is selected and aligned to the template(s) of the source pattern. This results in skipgram to skipgram alignments. N-gram to skipgram or skipgram to n-gram alignments are not supported by this method.</p>
<p>The algorithm is implemented as illustrated by the following pseudo-code:</p>
<div class="highlight-python"><div class="highlight"><pre> <span class="k">for</span> <span class="n">ngram_s</span> <span class="ow">in</span> <span class="n">alignmatrix</span><span class="p">:</span>
    <span class="k">if</span> <span class="o">|</span><span class="n">alignmatrix</span><span class="p">[</span><span class="n">ngram_s</span><span class="p">]</span><span class="o">|</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span> <span class="c">#(nothing to generalise otherwise)</span>
        <span class="n">skipgrams_s</span> <span class="o">=</span> <span class="n">get_templates_recursively</span><span class="p">(</span><span class="n">rel_templates_s</span><span class="p">[</span><span class="n">ngram_s</span><span class="p">])</span>
        <span class="k">if</span> <span class="n">skipgrams_s</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">ngram_t</span> <span class="ow">in</span> <span class="n">alignmatrix</span><span class="p">[</span><span class="n">ngram_s</span><span class="p">]</span>
                <span class="n">skipgrams_t</span> <span class="o">=</span> <span class="n">get_templates_recursively</span><span class="p">(</span><span class="n">rel_templates_t</span><span class="p">[</span><span class="n">ngram_t</span><span class="p">])</span>
                <span class="k">if</span> <span class="n">skipgrams_t</span><span class="p">:</span>
                    <span class="k">for</span> <span class="n">skipgram_s</span> <span class="ow">in</span> <span class="n">skipgrams_s</span><span class="p">:</span>
                        <span class="k">for</span> <span class="n">skipgram_t</span> <span class="ow">in</span> <span class="n">skipgrams_t</span><span class="p">:</span>
                            <span class="n">submatrix</span><span class="p">[</span><span class="n">skipgram_s</span><span class="p">][</span><span class="n">skipgram_t</span><span class="p">]</span><span class="o">++</span>
<span class="n">prune</span> <span class="nb">all</span> <span class="mi">1</span><span class="o">-</span><span class="n">values</span> <span class="ow">in</span> <span class="n">submatrix</span>

<span class="k">for</span> <span class="n">skipgram_s</span> <span class="ow">in</span> <span class="n">submatrix</span><span class="p">:</span> <span class="c">#conflict resolution</span>
    <span class="k">if</span> <span class="o">|</span><span class="n">submatrix</span><span class="p">[</span><span class="n">skipgram_s</span><span class="p">]</span><span class="o">|</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">skipgram_t</span> <span class="o">=</span> <span class="n">first</span> <span class="ow">and</span> <span class="n">only</span>
        <span class="n">alignmatrix</span><span class="p">[</span><span class="n">skipgram_s</span><span class="p">][</span><span class="n">skipgram_t</span><span class="p">]</span> <span class="o">=</span> <span class="n">submatrix</span><span class="p">[</span><span class="n">skipgram_s</span><span class="p">][</span><span class="n">skipgram_t</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">clusters</span> <span class="o">=</span> <span class="n">get_independent_cluster</span><span class="p">(</span><span class="n">skipgrams_t</span><span class="p">)</span> <span class="c">#complete subgraphs</span>
        <span class="k">for</span> <span class="n">cluster</span> <span class="ow">in</span> <span class="n">clusters</span><span class="p">:</span>
          <span class="n">maxcooc</span> <span class="o">=</span> <span class="mi">0</span>
          <span class="k">for</span> <span class="n">skipgram_t</span> <span class="ow">in</span> <span class="n">cluster</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">cooc</span><span class="p">(</span><span class="n">skipgram_s</span><span class="p">,</span> <span class="n">skipgram_t</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">maxcooc</span><span class="p">:</span>
                <span class="n">maxcooc</span> <span class="o">=</span> <span class="n">_</span>
                <span class="n">best</span> <span class="o">=</span> <span class="n">skipgram_t</span>
            <span class="k">elif</span>   <span class="o">==</span> <span class="n">maxcooc</span><span class="p">:</span>
               <span class="k">if</span> <span class="n">skipgram_t</span> <span class="n">more</span> <span class="n">abstract</span> <span class="n">than</span> <span class="n">best</span><span class="p">:</span>
                    <span class="n">best</span> <span class="o">=</span> <span class="n">skipgram_t</span>
          <span class="k">if</span> <span class="n">best</span><span class="p">:</span>
             <span class="n">alignmatrix</span><span class="p">[</span><span class="n">skipgram_s</span><span class="p">][</span><span class="n">best</span><span class="p">]</span> <span class="o">=</span> <span class="n">submatrix</span><span class="p">[</span><span class="n">skipgram_s</span><span class="p">][</span><span class="n">best</span><span class="p">]</span>
</pre></div>
</div>
<p>The following example takes an alignment model computed with only n-grams according to one of the three methods outlined earlier. It outputs a new alignment model, a superset of the original one, that adds the skipgram relations. The <tt class="docutils literal"><span class="pre">-U</span></tt> flag activates the skipgram extraction algorithm. Make sure to use the <tt class="docutils literal"><span class="pre">-o</span></tt> parameter to output a new model to a new file:</p>
<div class="highlight-python"><pre>$ aligner -U -d europarl25k-nl-en.alignmodel.colibri -o europarl25k-nl-en-withskipgrams.alignmodel.colibri</pre>
</div>
</div>
<div class="section" id="viewing-an-alignment-model">
<h3>Viewing an alignment model<a class="headerlink" href="#viewing-an-alignment-model" title="Permalink to this headline">¶</a></h3>
<p>An alignment model can be viewed by decoding it using the <tt class="docutils literal"><span class="pre">-d</span></tt> option and by passing it the class file for both source (<tt class="docutils literal"><span class="pre">-S</span></tt>) and target language (<tt class="docutils literal"><span class="pre">-T</span></tt>):</p>
<div class="highlight-python"><pre>$ aligner -d alignmodel.colibri -S fr.cls -T en.cls</pre>
</div>
<p>The output format is a simple tab-delimited format.</p>
<p>An alignment model can also be outputted in a format suitable for the MT decoder <em>moses</em> <a class="reference internal" href="#koehn2007">[Koehn2007]</a>:</p>
<div class="highlight-python"><pre>$ aligner -d alignmodel.colibri -S fr.cls -T en.cls --moses</pre>
</div>
</div>
</div>
<div class="section" id="mt-decoder">
<h2>MT Decoder<a class="headerlink" href="#mt-decoder" title="Permalink to this headline">¶</a></h2>
<div class="section" id="id8">
<h3>Introduction<a class="headerlink" href="#id8" title="Permalink to this headline">¶</a></h3>
<p>A good translation should faithfully convey the meaning of the original, and it should be rendered in fluent natural language. In statistical machine translation, these two aspects are represented by two distinct statistical models. The translation model is used to compute the likelihood of a sentence being faithful to the original meaning, and the target language model imposes a maximally fluent natural word order on the resulting translation in the target language by scoring typical, predictable word order as more probable than uncommon or malformed combinations. The central processing component of an SMT system is the decoder, which computes probabilities according to at least these two models for a huge number of possible translation hypotheses. This constitutes a vast search problem in which countless hypotheses are tested and compete against one another for the best probability score according to the joint statistical models. The translation chosen is the hypothesis found to attain the best score. Due to the size and complexity of the search problem, and the need to keep time and memory requirements manageable, considerable pruning of the search takes place. It is quite possible that the selected translation is found in a local maximum.</p>
<p>Colibri features a custom-tailored MT decoder modelled after the stack-based decoding principle used in Moses <a class="reference internal" href="#koehn2003">[Koehn2003]</a>. This algorithm employs a beam search as a means to constrain the search space. The alignment model as discussed in the previous chapter provide the translation model. The mandatory language model can be computed with third-party software such as <a class="reference external" href="http://www.speech.sri.com/projects/srilm/">SRILM</a> <a class="reference internal" href="#stolcke2002">[Stolcke2002]</a> .</p>
<p>The stack decoder maintains <em>n</em> separate stacks, the number of stacks equals the number of words in the <em>source</em> sentence. Each stack contains translation hypotheses that cover an arbitrary <em>n</em> words of the source sentence. Only a fixed number of the best translation hypotheses are retained, this is the <em>stack size</em>. In the decoding process proceeds iteratively, starting with an <em>empty hypothesis</em> for which no source words have been covered and thus no target words have been generated. This initial hypotheses is <em>expanded</em> by choosing each input pattern matching the source sentence, at any position, and translating it according to the translation model. The translations are added left-to-right, but the selected source are from any location. This makes reordering possible. This expansions in turn results in a larger number of new translation hypotheses which are added to the appropriate stacks. After the initial hypothesis, the first stack is processed top to bottom, expanding in turn each of the hypotheses in it, resulting in more hypotheses to be added to the higher stacks. When a stack is processed, the algorithm moves on to the next. The final stack will be the stack in which all source words are covered and thus in which the hypotheses represent translations of the whole sentence. The best one is selected as the translation.</p>
<p>The pseudo-code for the algorithm is as follows (from <a class="reference external" href="http://www.statmt.org/moses/?n=Moses.Background">Moses Background</a>) <a class="reference internal" href="#koehn2007">[Koehn2007]</a>:</p>
<div class="highlight-python"><pre>initialize hypothesisStack[0 .. nf];
create initial hypothesis hyp_init;
add to stack hypothesisStack[0];
for i=0 to nf-1:
  for each hyp in hypothesisStack[i]:
        for each new_hyp that can be derived from hyp:
          nf[new_hyp] = number of foreign words covered by new_hyp;
          add new_hyp to hypothesisStack[nf[new_hyp]];
          prune hypothesisStack[nf[new_hyp]];
find best hypothesis best_hyp in hypothesisStack[nf];
output best path that leads to best_hyp;</pre>
</div>
<p>The website <a class="reference external" href="http://www.statmt.org/moses/?n=Moses.Background">Moses Background</a> provides more detail about this algorithm. The algorithm implemented in colibri is identical, although hypothesis recombination is not implemented yet in the Colibi decoder.</p>
<p>The colibri decoder has been enhanced to be able to handle skipgrams. Source-side skipgrams need no special handling due to the stack-based nature that already supports it; translations with gaps however do need special attention. The colibri decoder solves this by introducing a second set of another <em>n</em> stacks for hypotheses containing gaps. The expansion operation for hypotheses in these stacks attempts explicitly seeks to fill one of gaps first, the expanded hypotheses will again form part of the normal <em>gapless</em> stacks once all gaps are filled.</p>
<p>Unlike Moses, the colibri decoder also integrates support for classifiers. This will be introduced in the next chapter.</p>
</div>
<div class="section" id="language-model">
<h3>Language Model<a class="headerlink" href="#language-model" title="Permalink to this headline">¶</a></h3>
<p>As a Language Model is a vital component of a machine translation system, here are instructions on how to generate a language model using <a class="reference external" href="http://www.speech.sri.com/projects/srilm/">SRILM</a> [Stolcke 2002]_. The language model should be generated for only the target language corpus:</p>
<div class="highlight-python"><pre>$ ngram-count -kndiscount -order 3 -unk -text yourtargetcorpus.txt -lm yourtargetcorpus.lm</pre>
</div>
<p>The above command generates a trigram model (<tt class="docutils literal"><span class="pre">-order</span> <span class="pre">3</span></tt>) with (modified) Knesser Ney discounting (<tt class="docutils literal"><span class="pre">-kndiscount</span></tt>), it includes supports for unknown words (<tt class="docutils literal"><span class="pre">-unk</span></tt>). Support for unknown words and back-off are requirements for the colibri decoder. The final argument specifies the output file for the language model, this can in turn be read by the colibri decoder.</p>
<p>Note that as this is external softare, you need to pass the <em>tokenised</em> corpus in plain-text format, not the class-encoded binary variant used by colibri. But do make sure both are generated on exactly the same data.</p>
</div>
<div class="section" id="decoder-usage">
<h3>Decoder Usage<a class="headerlink" href="#decoder-usage" title="Permalink to this headline">¶</a></h3>
<p>The colibri decoder requires at least an alignment model (also known as translation model), and a language model. The translation model may contain multiple scores. The final score for an hypothesis is a weighted log-linear combination of all components model, in which each model carries a weight. These weights are passed as parameters to the decoder. The <tt class="docutils literal"><span class="pre">-W</span></tt> argument specifies weights for the scores in the alignment model, it should be repeated for the amount of scores in a model. The <tt class="docutils literal"><span class="pre">-L</span></tt> argument sets the weight for the language model, <tt class="docutils literal"><span class="pre">-D</span></tt> does the same for the distortion model (both default to one). The following example shows how to invoke the decoder given a translation model (<tt class="docutils literal"><span class="pre">-t</span></tt>) and language model (<tt class="docutils literal"><span class="pre">-l</span></tt>). It is also necessary to specify the class file for both source (<tt class="docutils literal"><span class="pre">-S</span></tt>) and (<tt class="docutils literal"><span class="pre">-T</span></tt>) target:</p>
<div class="highlight-python"><pre>$ decoder -t alignmodel.colibri -l yourtargetcorpus.lm -S yoursourcecorpus.cls -T yourtargetcorpus.cls -W 1 -W 1 &lt; inputfile.txt</pre>
</div>
<p>The decoder reads its input from <tt class="docutils literal"><span class="pre">stdin</span></tt>, hence the `` &lt; inputfile.txt``. The input should consist of tokenised sentences in the source language, one sentence per line. It will output the translation to <tt class="docutils literal"><span class="pre">stdout</span></tt>, again one sentence per line. Extra information will be written to <tt class="docutils literal"><span class="pre">stderr</span></tt>, the verbosity of which can be adjusted with the <tt class="docutils literal"><span class="pre">-v</span></tt> parameter.</p>
<p>The decoder takes the following extra parameters:</p>
<ul class="simple">
<li><tt class="docutils literal"><span class="pre">-s</span></tt> - The stacksize (defaults to 100)</li>
<li><tt class="docutils literal"><span class="pre">-p</span></tt> - The prune threshold. The pruning threshold is a number between 0 and 1. It determines how much of a stack to prune based on the highest scoring hypothesis in the stack. If the prune threshold is set to for instance <em>0.8</em> then all hypotheses having a score of less than <em>80%</em> of the highest scoring hypothesis are pruned.</li>
<li><tt class="docutils literal"><span class="pre">-W</span> <span class="pre">n</span></tt> - Translation model weight. Issue this parameter multiple times if there are multiple scores in your alignment model (<tt class="docutils literal"><span class="pre">-W</span> <span class="pre">1</span> <span class="pre">-W</span> <span class="pre">1</span></tt>). Alignment models generated with <tt class="docutils literal"><span class="pre">aligner</span> <span class="pre">-I</span> <span class="pre">2</span></tt> generate two scores. If there is a mismatch between the number of scores specified here and in the alignment model, then an error will be raised.</li>
<li><tt class="docutils literal"><span class="pre">-L</span> <span class="pre">n</span></tt> - Language model weight.</li>
<li><tt class="docutils literal"><span class="pre">-D</span> <span class="pre">n</span></tt> - Distortion model weight.</li>
<li><tt class="docutils literal"><span class="pre">-N</span></tt> - Ignore and do not use skipgrams</li>
<li><tt class="docutils literal"><span class="pre">-M</span> <span class="pre">n</span></tt> -   Distortion limit. This corresponds to the maximum number of words a reordering displacement occurs over (default: unlimited).</li>
</ul>
<p>In addition, the following parameters control output and verbosity:</p>
<ul class="simple">
<li><tt class="docutils literal"><span class="pre">-v</span> <span class="pre">n</span></tt> - Sets the verbosity value. <em>n</em> is a number from 0 to 5. Setting a high level will very explicitly show what the decoder is doing, be aware that this generates a huge amount of output to <tt class="docutils literal"><span class="pre">stderr</span></tt>.</li>
<li><tt class="docutils literal"><span class="pre">--stats</span></tt> - Compute and output decoding statistics for each solution.</li>
<li><tt class="docutils literal"><span class="pre">--globalstats</span></tt> - Compute and output decoding statistics for all hypothesis accepted on a stack.</li>
</ul>
</div>
</div>
<div class="section" id="machine-learning">
<h2>Machine Learning<a class="headerlink" href="#machine-learning" title="Permalink to this headline">¶</a></h2>
<p>(yet to be written)</p>
</div>
<div class="section" id="mt-experiment-framework">
<h2>MT Experiment Framework<a class="headerlink" href="#mt-experiment-framework" title="Permalink to this headline">¶</a></h2>
<p>(yet to be written)</p>
</div>
<div class="section" id="references">
<h2>References<a class="headerlink" href="#references" title="Permalink to this headline">¶</a></h2>
<table class="docutils citation" frame="void" id="daelemans2010" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id1">[Daelemans2010]</a></td><td>Daelemans, W., Zavrel, J., Van der Sloot, K., and Van den Bosch, A. (2010). TiMBL: Tilburg Memory Based Learner, version 6.3, Reference Guide. ILK Research Group Technical Report Series no. 10-01.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="koehn2003" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id9">[Koehn2003]</a></td><td><ol class="first last upperalpha simple" start="16">
<li>Koehn, F.J Och, D. Marcu, <em>Statistical phrase-based translation</em>, NAACL &#8216;03 Proceedings of the 2003. Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology - Volume 1 (2003) pp. 48-54.</li>
</ol>
</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="koehn2007" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[Koehn2007]</td><td><em>(<a class="fn-backref" href="#id7">1</a>, <a class="fn-backref" href="#id11">2</a>)</em> <ol class="last upperalpha simple" start="16">
<li>Koehn, H. Hoang, A. Birch, C. Callison-Burch, M. Federico, N. Bertoldi, B. Cowan, W. Shen, C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin and E. Herbst, <em>Moses: Open source toolkit for statistical machine translation</em>, ACL &#8216;07 Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions. (2007) pp. 177-180.</li>
</ol>
</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="ochney2003" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id6">[OchNey2003]</a></td><td>F.J Och and H. Ney, <em>A Systematic Comparison of Various Statistical Alignment Models</em>, Computational Linguistics, (2003) volume 29, number 1, pp. 19-51</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="stolcke2002" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id10">[Stolcke2002]</a></td><td><ol class="first last upperalpha simple">
<li>Stolcke, <em>SRILM — an extensible language modeling toolkit</em>, in J. H. L. Hansen and B. Pellom, editors, Proc. ICSLP, vol. 2, pp. 901–904, Denver, Sep. 2002.</li>
</ol>
</td></tr>
</tbody>
</table>
</div>
</div>


          </div>
        </div>
      </div>
      <div class="sphinxsidebar">
        <div class="sphinxsidebarwrapper">
  <h3><a href="#">Table Of Contents</a></h3>
  <ul>
<li><a class="reference internal" href="#">Colibri Documentation</a><ul>
<li><a class="reference internal" href="#introduction">Introduction</a></li>
<li><a class="reference internal" href="#installation">Installation</a><ul>
<li><a class="reference internal" href="#keeping-colibri-up-to-date">Keeping colibri up to date</a></li>
<li><a class="reference internal" href="#general-usage-instructions">General usage instructions</a></li>
</ul>
</li>
<li><a class="reference internal" href="#corpus-class-encoding">Corpus Class Encoding</a><ul>
<li><a class="reference internal" href="#id2">Introduction</a></li>
<li><a class="reference internal" href="#class-encoding-your-corpus">Class-encoding your corpus</a></li>
<li><a class="reference internal" href="#class-decoding-your-corpus">Class-decoding your corpus</a></li>
<li><a class="reference internal" href="#class-encoding-with-existing-classes">Class-encoding with existing classes</a></li>
</ul>
</li>
<li><a class="reference internal" href="#pattern-finder">Pattern Finder</a><ul>
<li><a class="reference internal" href="#id3">Introduction</a></li>
<li><a class="reference internal" href="#creating-a-pattern-model">Creating a pattern model</a></li>
<li><a class="reference internal" href="#training-and-testing-coverage">Training and testing coverage</a></li>
<li><a class="reference internal" href="#query-mode">Query mode</a></li>
</ul>
</li>
<li><a class="reference internal" href="#graph-models">Graph Models</a><ul>
<li><a class="reference internal" href="#id4">Introduction</a></li>
<li><a class="reference internal" href="#computing-a-graph-model">Computing a graph model</a></li>
<li><a class="reference internal" href="#viewing-and-querying-a-graph-model">Viewing and querying a graph model</a></li>
</ul>
</li>
<li><a class="reference internal" href="#alignment-models">Alignment Models</a><ul>
<li><a class="reference internal" href="#id5">Introduction</a></li>
<li><a class="reference internal" href="#co-occurrence">Co-occurrence</a></li>
<li><a class="reference internal" href="#expectation-maximisation">Expectation Maximisation</a></li>
<li><a class="reference internal" href="#giza-alignment">GIZA Alignment</a></li>
<li><a class="reference internal" href="#skipgram-alignment">Skipgram alignment</a></li>
<li><a class="reference internal" href="#viewing-an-alignment-model">Viewing an alignment model</a></li>
</ul>
</li>
<li><a class="reference internal" href="#mt-decoder">MT Decoder</a><ul>
<li><a class="reference internal" href="#id8">Introduction</a></li>
<li><a class="reference internal" href="#language-model">Language Model</a></li>
<li><a class="reference internal" href="#decoder-usage">Decoder Usage</a></li>
</ul>
</li>
<li><a class="reference internal" href="#machine-learning">Machine Learning</a></li>
<li><a class="reference internal" href="#mt-experiment-framework">MT Experiment Framework</a></li>
<li><a class="reference internal" href="#references">References</a></li>
</ul>
</li>
</ul>

  <h3>This Page</h3>
  <ul class="this-page-menu">
    <li><a href="_sources/index.txt"
           rel="nofollow">Show Source</a></li>
  </ul>
<div id="searchbox" style="display: none">
  <h3>Quick search</h3>
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" />
      <input type="submit" value="Go" />
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
    <p class="searchtip" style="font-size: 90%">
    Enter search terms or a module, class or function name.
    </p>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             >index</a></li>
        <li><a href="#">Colibri 0.1 documentation</a> &raquo;</li> 
      </ul>
    </div>
    <div class="footer">
        &copy; Copyright 2012, Maarten van Gompel.
      Created using <a href="http://sphinx.pocoo.org/">Sphinx</a> 1.1.3.
    </div>
  </body>
</html>